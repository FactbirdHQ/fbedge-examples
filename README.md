# Edge AI Examples with Local Storage

This repository provides comprehensive examples for getting started with Hailo8 edge devices, including video streaming, model training, compilation, and local deployment. AWS integration is available as an optional feature.

## ğŸ“ Project Structure

```
fbedge-examples/
â”œâ”€â”€ examples.ipynb              # Main Jupyter notebook with complete workflow
â”œâ”€â”€ video_capture.py            # KVS stream consumption and data capture
â”œâ”€â”€ iot_deployment.py           # AWS IoT job creation for edge deployment
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ setup.py                    # Package setup (setuptools)
â”œâ”€â”€ pyproject.toml             # Modern Python packaging configuration
â”œâ”€â”€ requirements.txt            # Python package dependencies
â”œâ”€â”€ MANIFEST.in                 # Package manifest for distribution
â”œâ”€â”€ .env.example               # Example environment configuration
â”œâ”€â”€ CLAUDE.md                   # Claude Code assistant instructions
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ .env                        # AWS configuration (created by setup)
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd fbedge-examples

# Option A: Install as editable package (recommended)
pip install -e .

# Option B: Install with extras
pip install -e .[ml]         # Machine learning dependencies
pip install -e .[dev]        # Development tools (pytest, black)
pip install -e .[jupyter]    # Jupyter notebook support
pip install -e .[aws]        # AWS integration (optional)

# Option C: Traditional requirements file
pip install -r requirements.txt

# Optional: Install Hailo Platform SDK (from Hailo)
# pip install hailo-platform-sdk
```

### 2. Quick Setup

After installation, you can use the command-line tool:

```bash
# Run the setup wizard
fbedge-setup

# Or copy the example environment file
cp .env.example .env
# Then edit .env with your settings
```

### 3. Data Storage Setup

Data is organized in structured directories:
- Raw frames: `data/raw/{stream_id}/{timestamp}/`
- Processed data: `data/processed/calibration-set/`
- Models: `models/onnx/`, `models/hef/`, `models/checkpoints/`

### 4. KVS Stream Data Capture

```python
from video_capture import test_aws_connection, setup_kvs_stream, KVSStreamConsumer

# Test AWS connection
aws_connected, aws_session = test_aws_connection(AWS_CONFIG)

# Setup KVS stream
kvs_config = setup_kvs_stream(aws_session, "your_stream_id", AWS_CONFIG)

# Consume video data from edge device
consumer = KVSStreamConsumer(aws_session, kvs_config)
success = consumer.consume_stream(session_dir, stream_config)
```

### 5. IoT Deployment

```python
from iot_deployment import check_iot_thing_exists, create_deployment_job

# Check if IoT thing exists
thing_exists, thing_info = check_iot_thing_exists(aws_session, "thing_id")

# Create download job for edge device
job_created, job_id, job_arn = create_deployment_job(
    aws_session,
    thing_info['thingArn'],
    "https://your-download-url.com/model.hef"
)
```

## ğŸ”§ Core Modules

### `video_capture.py`

**AWS Kinesis Video Streams integration and data capture**

- `test_aws_connection()`: Test AWS credentials and connectivity
- `setup_data_directories()`: Create organized data storage structure
- `setup_kvs_stream()`: Configure KVS stream connections
- `KVSStreamConsumer`: Consume video streams from edge devices
- Stream identification with Stream ID
- Backward consumption from KVS streams
- Frame extraction and local storage

### `iot_deployment.py`

**AWS IoT Core integration for edge device deployment**

- `check_iot_thing_exists()`: Verify IoT thing registration
- `create_deployment_job()`: Create download jobs for edge devices
- `check_job_status()`: Monitor deployment job progress
- `list_deployment_jobs()`: List recent deployment jobs
- `cancel_deployment_job()`: Cancel active jobs
- Download job document creation for edge device communication

## ğŸ’¾ Local Storage Organization

### Data Directory Structure
```
data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ {stream_id}/
â”‚       â””â”€â”€ {YYYYMMDD_HHMMSS}/
â”‚           â”œâ”€â”€ frame_001.jpg
â”‚           â”œâ”€â”€ frame_002.jpg
â”‚           â””â”€â”€ session_metadata.json
â””â”€â”€ processed/
    â””â”€â”€ calibration-set/
        â”œâ”€â”€ image_001.jpg
        â””â”€â”€ image_002.jpg
```

### Models Directory Structure
```
models/
â”œâ”€â”€ onnx/
â”‚   â””â”€â”€ model.onnx
â”œâ”€â”€ hef/
â”‚   â””â”€â”€ model.hef
â””â”€â”€ checkpoints/
    â””â”€â”€ best.pt
```

## ğŸŒ Optional AWS Integration Features

When using the `[aws]` extra, you can enable:
- S3 uploads for data storage
- IoT Core messaging for real-time updates
- Cloud-based monitoring and analytics

## ğŸ“Š Environment Configuration

Create a `.env` file with:

```bash
# Stream Configuration
STREAM_ID=video_stream_001

# Video Capture Settings
DEFAULT_VIDEO_SOURCE=0
TARGET_FPS=5
CAPTURE_DURATION=60
CONFIDENCE_THRESHOLD=0.7

# Data Storage Paths
DATA_DIR=./data
MODELS_DIR=./models

# Optional: AWS Integration (if using [aws] extra)
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key
# AWS_REGION=us-west-2
# S3_BUCKET=video-streaming-datasets
# IOT_ENDPOINT=your-iot-endpoint.iot.us-west-2.amazonaws.com
```

## âš™ï¸ Model Compilation Prerequisites

**IMPORTANT**: Model compilation to HEF format requires specific environment setup:

### Requirements
- **Operating System**: Linux x86_64 only
- **Hailo Dataflow Compiler (DFC)**: Must be installed and available
- **Python Environment**: Same virtual environment used for training

### Setup Instructions
1. **Linux x86 Environment**: Compilation only works on Linux x86_64 systems
2. **Install Hailo DFC**: Follow Hailo's official installation guide for the Dataflow Compiler
3. **Virtual Environment**: Use the same Python virtual environment that was used for model training:
   ```bash
   # Activate the training environment
   source your-training-venv/bin/activate

   # Verify Hailo DFC installation
   hailo -h
   ```
4. **Prerequisites**: Ensure all training dependencies are available in the same environment

### Note
If you don't have access to a Linux x86 system with Hailo DFC, you can:
- Use Hailo's cloud compilation services
- Contact Hailo support for compilation assistance
- Use pre-compiled HEF models for testing

## ğŸ¯ Complete Workflow

1. **Setup**: Configure Stream ID and AWS credentials for KVS integration
2. **Data Capture**: Consume video streams from edge devices via AWS Kinesis Video Streams
3. **Data Annotation**: Annotate captured frames using Roboflow, CVAT, or other tools
4. **Training**: Train models using Hailo Model Zoo Docker environment
5. **Compilation**: Convert ONNX â†’ HAR â†’ optimized HAR â†’ HEF (requires Hailo DFC on Linux x86)
6. **Deployment**: Create AWS IoT jobs to deploy models to edge devices

## ğŸ“ Example Notebooks

The main `examples.ipynb` notebook includes:
- AWS configuration and credential setup
- KVS stream setup and video data consumption
- Data annotation guidelines and tool recommendations
- Hailo Model Zoo training workflow
- Complete compilation pipeline (ONNX â†’ HAR â†’ optimized HAR â†’ HEF)
- AWS IoT job creation for edge device deployment
- Deployment monitoring and status checking

## ğŸ†˜ Troubleshooting

### Common Issues

1. **AWS Connection Issues**
   - Check your AWS credentials are current (short-lived tokens expire)
   - Verify IAM permissions for KVS, IoT Core, and STS
   - Test with `test_aws_connection()` function

2. **KVS Stream Issues**
   - Ensure your edge device is actively streaming to KVS
   - Check stream exists with correct Stream ID
   - Verify stream is in ACTIVE status

3. **IoT Deployment Issues**
   - Verify IoT thing is registered in AWS IoT Core
   - Check thing ARN matches your device
   - Ensure deployment URL is publicly accessible

4. **Compilation Issues**
   - Requires Linux x86_64 system with Hailo DFC
   - Use same virtual environment as training
   - Verify all training dependencies are available

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with both real and mock Hailo implementations
5. Submit a pull request

## ğŸ“ Support

- Check the Hailo Developer Zone for SDK documentation
- AWS documentation for IoT Core and S3 setup
- Create issues for bugs or feature requests